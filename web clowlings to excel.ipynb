{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "title_list = []\n",
    "url_list = []\n",
    "\n",
    "url = \"https://r.nikkei.com/search?keyword=m%26a&volume=200\"\n",
    "\n",
    "html_doc = requests.get(url)\n",
    "soup = BeautifulSoup(html_doc.content, 'html.parser') # BeautifulSoupの初期化\n",
    "tags = soup.find_all('h3',{'class':'nui-card__title'})\n",
    "\n",
    "for tag in tags:\n",
    "        title_list.append(tag.text.replace(\"\\n\",\"\"))\n",
    "        url_list.append(tag.select(\"a\")[0].get(\"href\"))\n",
    "        \n",
    "        \n",
    "my_df = pd.DataFrame.from_dict({\"title\": title_list, \"url\": url_list})\n",
    "\n",
    "title_list2 = []\n",
    "url_list2 = []\n",
    "\n",
    "url2 = \"https://news.yahoo.co.jp/search/?ei=UTF-8&p=M%26a&st=n\"\n",
    "\n",
    "html_doc2 = requests.get(url2)\n",
    "soup2 = BeautifulSoup(html_doc2.content, 'html.parser') # BeautifulSoupの初期化\n",
    "tags2 = soup2.find_all('h2',{'class':'t'})\n",
    "\n",
    "for tag2 in tags2:\n",
    "        title_list2.append(tag2.text.replace(\"\\n\",\"\"))\n",
    "        url_list2.append(tag2.select(\"a\")[0].get(\"href\"))\n",
    "        \n",
    "for number in range(2,10):\n",
    "    page_tag2 = soup2.find('a',{'title':number})\n",
    "    page_tag2_url = page_tag2.get(\"href\")\n",
    "    responce = requests.get(page_tag2_url)\n",
    "    soup3 = BeautifulSoup(responce.content, 'html.parser') \n",
    "    tags3 = soup3.find_all('h2',{'class':'t'})\n",
    "    for tag3 in tags3:\n",
    "        title_list2.append(tag3.text.replace(\"\\n\",\"\"))\n",
    "        url_list2.append(tag3.select(\"a\")[0].get(\"href\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "my_df2 = pd.DataFrame.from_dict({\"title\": title_list2, \"url\": url_list2})\n",
    "\n",
    "title_list3 = []\n",
    "time_list3 = []\n",
    "text_list3 = []\n",
    "link_list3 = []\n",
    "url_list3 = []\n",
    "\n",
    "#Access to page\n",
    "url3 = \"https://maonline.jp/articles\"\n",
    "page_1 = requests.get(url3)\n",
    "soup3 = BeautifulSoup(page_1.content, 'html.parser') # BeautifulSoupの初期化\n",
    "tags3 = soup3.find_all('div',{'class':'list new'})\n",
    "\n",
    "for tag3 in tags3:\n",
    "    title_list3.append(tag3.find('h3').text)\n",
    "    time_list3.append(tag3.find('time').text)\n",
    "    text_list3.append(tag3.find('p').text)\n",
    "    link_list3.append(tag3.find('a').get('href'))\n",
    "\n",
    "for number in range(2,15):\n",
    "    url_list3.append(\"https://maonline.jp/articles?page=\" + str(number))\n",
    "\n",
    "for url4 in url_list3:\n",
    "    pages_2 = requests.get(url4)\n",
    "    soup4 = BeautifulSoup(pages_2.content, 'html.parser')\n",
    "    tags4 = soup4.find_all('div',{'class':re.compile(\"^list\")})\n",
    "    for tag4 in tags4:\n",
    "        title_list3.append(tag4.find('h3').text)\n",
    "        time_list3.append(tag4.find('time').text)\n",
    "        text_list3.append(tag4.find('p').text)\n",
    "        link_list3.append(tag4.find('a').get('href'))\n",
    "    \n",
    "pd.set_option('display.max_rows', 500)        \n",
    "my_df3 = pd.DataFrame.from_dict({\"title\": title_list3,\"time\": time_list3,\"text\": text_list3,\"link\": link_list3})\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(r\"C:\\Program Files\\Python37\\DLLs\\nikkei.xlsx\") as writer:\n",
    "    my_df.to_excel(writer, sheet_name='sheet1')\n",
    "    my_df2.to_excel(writer, sheet_name='sheet2')\n",
    "    my_df3.to_excel(writer, sheet_name='sheet3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "title_list3 = []\n",
    "time_list3 = []\n",
    "text_list3 = []\n",
    "link_list3 = []\n",
    "url_list3 = []\n",
    "\n",
    "#Access to page\n",
    "url3 = \"https://maonline.jp/articles\"\n",
    "page_1 = requests.get(url3)\n",
    "soup3 = BeautifulSoup(page_1.content, 'html.parser') # BeautifulSoupの初期化\n",
    "tags3 = soup3.find_all('div',{'class':'list new'})\n",
    "\n",
    "for tag3 in tags3:\n",
    "    title_list3.append(tag3.find('h3').text)\n",
    "    time_list3.append(tag3.find('time').text)\n",
    "    text_list3.append(tag3.find('p').text)\n",
    "    link_list3.append(tag3.find('a').get('href'))\n",
    "\n",
    "for number in range(2,15):\n",
    "    url_list3.append(\"https://maonline.jp/articles?page=\" + str(number))\n",
    "\n",
    "for url4 in url_list3:\n",
    "    pages_2 = requests.get(url4)\n",
    "    soup4 = BeautifulSoup(pages_2.content, 'html.parser')\n",
    "    tags4 = soup4.find_all('div',{'class':re.compile(\"^list\")})\n",
    "    for tag4 in tags4:\n",
    "        title_list3.append(tag4.find('h3').text)\n",
    "        time_list3.append(tag4.find('time').text)\n",
    "        text_list3.append(tag4.find('p').text)\n",
    "        link_list3.append(tag4.find('a').get('href'))\n",
    "    \n",
    "pd.set_option('display.max_rows', 500)        \n",
    "my_df3 = pd.DataFrame.from_dict({\"title\": title_list3,\"time\": time_list3,\"text\": text_list3,\"link\": link_list3})\n",
    "\n",
    "import gspread_dataframe as gd\n",
    "\n",
    "# Connecting with `gspread` here\n",
    "\n",
    "ws = gc.open(\"pythonで読み書き\").worksheet(\"シート1\")\n",
    "gd.set_with_dataframe(ws,my_df3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
